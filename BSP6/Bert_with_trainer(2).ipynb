{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_liMema38L3",
        "outputId": "693a7dcf-f388-49c4-e64a-b424fd0f244e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jun 11 17:53:33 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y_chKtu3m57i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f002e783-90f6-4751-fe9e-664a33ad3e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "#install missing dependancies\n",
        "!pip install transformers accelerate\n",
        "\n",
        "\n",
        "#Library used for fine tuning\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Pandas Dataframe Library\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from IPython.display import display\n",
        "import numpy as np \n",
        "import datetime\n",
        "\n",
        "\n",
        "# HuggingFace Libarary\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, BertConfig, BertTokenizer\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "\n",
        "\n",
        "#All tested models \n",
        "bert_cased=\"bert-base-cased\"\n",
        "bert=\"bert-base-uncased\"\n",
        "HateBert=\"GroNLP/hateBERT\"\n",
        "DistilBert=\"distilbert-base-uncased\"\n",
        "RoBERTa=\"roberta-base\" \n",
        "HateRoBERTa=\"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
        "\n",
        "\n",
        "# Model HyperParameters\n",
        "current_model=RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpP4lSMXP3Tx",
        "outputId": "1260a9f1-535a-4ebc-8187-ff849cbdf111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.getcwd()\n",
        "gold='/content/drive/MyDrive/BSP6/init_dataset_gold'\n",
        "gold_pre_processed = '/content/drive/MyDrive/BSP6/init_dataset_gold_preprocessed' \n",
        "gold_oversampeld ='/content/drive/MyDrive/BSP6/init_dataset_gold_silver_balance'\n",
        "gold_undersampeld ='/content/drive/MyDrive/BSP6/init_dataset_gold_balance'\n",
        "gold_oversample_pre='/content/drive/MyDrive/BSP6/init_dataset_gold_preprocessed_oversample'\n",
        "\n",
        "\n",
        "current_dataset=gold_oversample_pre\n",
        "os.chdir(current_dataset) \n",
        "now = str(datetime.datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mC0xDskLYPl9"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_json('train.json', lines=True)\n",
        "val_data = pd.read_json('val.json', lines=True)\n",
        "test_data = pd.read_json('test.json', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9p_XDqHascm",
        "outputId": "07a91006-e319-4947-cac5-975c74911fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# load pre-trained HateBert\n",
        "if(current_model==\"roberta-base\"):\n",
        "  model = AutoModelForSequenceClassification.from_pretrained('roberta-base',num_labels=3)\n",
        "  tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "elif(current_model==\"facebook/roberta-hate-speech-dynabench-r4-target\"):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\",num_labels=3, ignore_mismatched_sizes=True,hidden_dropout_prob=0.3,attention_probs_dropout_prob=0.3)\n",
        "else:\n",
        "  model = BertForSequenceClassification.from_pretrained(current_model,num_labels=3 )\n",
        "  tokenizer = BertTokenizer.from_pretrained(current_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAdrGJ6-mhJq",
        "outputId": "51a9e972-683f-42bd-b31b-e8818522ba7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiEuOi7MnW8E",
        "outputId": "7e743cd9-e9fa-4be1-f849-f34a6caca3e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Train Data\n",
        "#max length var\n",
        "max_length_X=225\n",
        "max_length_V=225\n",
        "max_length_T=225\n",
        "\n",
        "#truncation var\n",
        "tru = True\n",
        "\n",
        "X = []\n",
        "for context, target in zip(train_data.context, train_data.target):\n",
        "  X.append(tokenizer(context, target, padding='max_length', truncation=tru, max_length=max_length_X))\n",
        "y_train = list(train_data.label)\n",
        "print(X)\n",
        "X_train_tokenized = X\n",
        "\n",
        "\n",
        "#val Data\n",
        "V = []\n",
        "for context, target in zip(val_data.context, val_data.target):\n",
        "   V.append(tokenizer( context,target, padding='max_length', truncation=tru, max_length=max_length_V))\n",
        "y_val = list(val_data.label)\n",
        "V_val_tokenized = V\n",
        "\n",
        "#test Data\n",
        "T = []\n",
        "for context, target in zip(test_data.context, test_data.target):\n",
        "   T.append(tokenizer(context, target, padding='max_length', truncation=tru, max_length=max_length_T))\n",
        "y_test = list(test_data.label)\n",
        "T_test_tokenized = T\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNyY_Yvkds0e",
        "outputId": "7117b2ff-afb5-424b-9a97-022612bf6f49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>The UK is fucked.</s></s>>The ~~UK~~ world is fucked  FTFY</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "{'input_ids': [0, 133, 987, 16, 42647, 4, 2, 2, 15698, 133, 1437, 48256, 10494, 48256, 232, 16, 42647, 1437, 274, 20249, 975, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
            "225\n",
            "2\n",
            "<s>Listen to this wisdom.</s></s>Where the Fuck did you get that up arrow?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "{'input_ids': [0, 32917, 7, 42, 12320, 4, 2, 2, 13841, 5, 43774, 222, 47, 120, 14, 62, 27899, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
            "225\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "#print the token\n",
        "print(tokenizer.decode(X_train_tokenized[0].input_ids))\n",
        "print(X_train_tokenized[0])\n",
        "print(len(X_train_tokenized[0].input_ids))\n",
        "print(y_train[0])\n",
        "\n",
        "\n",
        "print(tokenizer.decode(X_train_tokenized[1].input_ids))\n",
        "print(X_train_tokenized[1])\n",
        "print(len(X_train_tokenized[1].input_ids))\n",
        "print(y_train[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nvXRE7Zyq3Hk"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "\n",
        "# Create torch dataset\n",
        "#need a dictionary\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {'input_ids':torch.tensor(self.encodings[idx].input_ids),\n",
        "                'attention_mask':torch.tensor(self.encodings[idx].attention_mask),\n",
        "                'labels':torch.tensor(self.labels[idx])}\n",
        "        \n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings)\n",
        "\n",
        "train_dataset = Dataset(X_train_tokenized, y_train)\n",
        "val_dataset = Dataset(V_val_tokenized, y_val)\n",
        "test_dataset = Dataset(T_test_tokenized, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZquUS8w_zeJy",
        "outputId": "09f32939-2f85-4688-eacd-ea274b6b7f5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([    0,   133,   987,    16, 42647,     4,     2,     2, 15698,   133,\n",
              "          1437, 48256, 10494, 48256,   232,    16, 42647,  1437,   274, 20249,\n",
              "           975,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'labels': tensor(2)}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HyKLuU49r47l"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "#importing confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "f = open(current_dataset+\"/Report/output\"+now+\".txt\",\"w\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    confusion = confusion_matrix(labels, preds)\n",
        "    print('Confusion Matrix\\n')\n",
        "    print(confusion)\n",
        "    print('\\nClassification Report\\n')\n",
        "    print(classification_report(labels, preds, target_names=['Hate', 'Neutral', 'Counter']))\n",
        "    f.write('Confusion Matrix\\n')\n",
        "    f.write(str(confusion))\n",
        "    f.write('\\nClassification Report\\n')\n",
        "    f.write(str(classification_report(labels, preds, target_names=['Hate', 'Neutral', 'Counter'])))\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgsI7FP7ehSx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VNWJ5H8zsW34"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define Trainer\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=5,               # total number of training epochs\n",
        "    per_device_train_batch_size=32,   # batch size per device during training\n",
        "    per_device_eval_batch_size=32,    # batch size for evaluation\n",
        "    learning_rate = 2e-5,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_steps = 50,\n",
        "    weight_decay=0.1,\n",
        "    load_best_model_at_end=True,\n",
        "    overwrite_output_dir=True,\n",
        "    seed=42\n",
        ")            \n",
        "\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "f.write(\"model\"+ current_model)\n",
        "f.write(\"dataset: \"+ current_dataset)\n",
        "f.write(\"\\n\")\n",
        "f.write(\"structure of the model: \\n\")\n",
        "f.write(str(trainer.model))\n",
        "f.write(\"\\n\")\n",
        "f.write(\"Tokenizer max length train:\"+str(max_length_X))\n",
        "f.write(\"Tokenizer max length val:\"+str(max_length_V))\n",
        "f.write(\"Tokenizer max length test:\"+str(max_length_T))\n",
        "f.write(\"\\n\")\n",
        "f.write(str(trainer.args))\n",
        "f.write(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSvA9sZsuf17",
        "outputId": "d6f0a5c9-eb14-4854-d40e-e4fb9dabcb80"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aD2mGaXWtfy1",
        "outputId": "6a26c146-e47c-4164-c363-f8b137924a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='665' max='665' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [665/665 16:19, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.096900</td>\n",
              "      <td>1.087224</td>\n",
              "      <td>0.380342</td>\n",
              "      <td>0.144660</td>\n",
              "      <td>0.380342</td>\n",
              "      <td>0.209600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.071800</td>\n",
              "      <td>1.051747</td>\n",
              "      <td>0.436966</td>\n",
              "      <td>0.377486</td>\n",
              "      <td>0.436966</td>\n",
              "      <td>0.350568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.999600</td>\n",
              "      <td>1.002014</td>\n",
              "      <td>0.530983</td>\n",
              "      <td>0.531134</td>\n",
              "      <td>0.530983</td>\n",
              "      <td>0.512277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.928900</td>\n",
              "      <td>1.004758</td>\n",
              "      <td>0.518162</td>\n",
              "      <td>0.523120</td>\n",
              "      <td>0.518162</td>\n",
              "      <td>0.515887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.912700</td>\n",
              "      <td>0.979216</td>\n",
              "      <td>0.511752</td>\n",
              "      <td>0.511514</td>\n",
              "      <td>0.511752</td>\n",
              "      <td>0.511617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.829000</td>\n",
              "      <td>1.006334</td>\n",
              "      <td>0.553419</td>\n",
              "      <td>0.557830</td>\n",
              "      <td>0.553419</td>\n",
              "      <td>0.554259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.811200</td>\n",
              "      <td>1.001195</td>\n",
              "      <td>0.524573</td>\n",
              "      <td>0.544561</td>\n",
              "      <td>0.524573</td>\n",
              "      <td>0.526223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.788100</td>\n",
              "      <td>0.998448</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.539737</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.534633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.680300</td>\n",
              "      <td>1.039732</td>\n",
              "      <td>0.560897</td>\n",
              "      <td>0.556378</td>\n",
              "      <td>0.560897</td>\n",
              "      <td>0.557027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.649600</td>\n",
              "      <td>1.071253</td>\n",
              "      <td>0.568376</td>\n",
              "      <td>0.561078</td>\n",
              "      <td>0.568376</td>\n",
              "      <td>0.555155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.621800</td>\n",
              "      <td>1.115289</td>\n",
              "      <td>0.540598</td>\n",
              "      <td>0.557387</td>\n",
              "      <td>0.540598</td>\n",
              "      <td>0.544405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.559300</td>\n",
              "      <td>1.113759</td>\n",
              "      <td>0.557692</td>\n",
              "      <td>0.553399</td>\n",
              "      <td>0.557692</td>\n",
              "      <td>0.552841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.574800</td>\n",
              "      <td>1.123878</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.559754</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.556673</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            "\n",
            "[[  0 331   0]\n",
            " [  0 356   0]\n",
            " [  0 249   0]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate       0.00      0.00      0.00       331\n",
            "     Neutral       0.38      1.00      0.55       356\n",
            "     Counter       0.00      0.00      0.00       249\n",
            "\n",
            "    accuracy                           0.38       936\n",
            "   macro avg       0.13      0.33      0.18       936\n",
            "weighted avg       0.14      0.38      0.21       936\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            "\n",
            "[[300  31   0]\n",
            " [247 109   0]\n",
            " [215  34   0]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate       0.39      0.91      0.55       331\n",
            "     Neutral       0.63      0.31      0.41       356\n",
            "     Counter       0.00      0.00      0.00       249\n",
            "\n",
            "    accuracy                           0.44       936\n",
            "   macro avg       0.34      0.40      0.32       936\n",
            "weighted avg       0.38      0.44      0.35       936\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            "\n",
            "[[251  59  21]\n",
            " [121 184  51]\n",
            " [125  62  62]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate       0.51      0.76      0.61       331\n",
            "     Neutral       0.60      0.52      0.56       356\n",
            "     Counter       0.46      0.25      0.32       249\n",
            "\n",
            "    accuracy                           0.53       936\n",
            "   macro avg       0.52      0.51      0.50       936\n",
            "weighted avg       0.53      0.53      0.51       936\n",
            "\n",
            "Confusion Matrix\n",
            "\n",
            "[[152  92  87]\n",
            " [ 55 232  69]\n",
            " [ 49  99 101]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate       0.59      0.46      0.52       331\n",
            "     Neutral       0.55      0.65      0.60       356\n",
            "     Counter       0.39      0.41      0.40       249\n",
            "\n",
            "    accuracy                           0.52       936\n",
            "   macro avg       0.51      0.51      0.50       936\n",
            "weighted avg       0.52      0.52      0.52       936\n",
            "\n",
            "Confusion Matrix\n",
            "\n",
            "[[182  72  77]\n",
            " [ 81 201  74]\n",
            " [ 73  80  96]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate       0.54      0.55      0.55       331\n",
            "     Neutral       0.57      0.56      0.57       356\n",
            "     Counter       0.39      0.39      0.39       249\n",
            "\n",
            "    accuracy                           0.51       936\n",
            "   macro avg       0.50      0.50      0.50       936\n",
            "weighted avg       0.51      0.51      0.51       936\n",
            "\n",
            "Confusion Matrix\n",
            "\n",
            "[[210  58  63]\n",
            " [ 72 195  89]\n",
            " [ 76  60 113]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate       0.59      0.63      0.61       331\n",
            "     Neutral       0.62      0.55      0.58       356\n",
            "     Counter       0.43      0.45      0.44       249\n",
            "\n",
            "    accuracy                           0.55       936\n",
            "   macro avg       0.55      0.55      0.54       936\n",
            "weighted avg       0.56      0.55      0.55       936\n",
            "\n",
            "Confusion Matrix\n",
            "\n",
            "[[144  80 107]\n",
            " [ 44 222  90]\n",
            " [ 42  82 125]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate       0.63      0.44      0.51       331\n",
            "     Neutral       0.58      0.62      0.60       356\n",
            "     Counter       0.39      0.50      0.44       249\n",
            "\n",
            "    accuracy                           0.52       936\n",
            "   macro avg       0.53      0.52      0.52       936\n",
            "weighted avg       0.54      0.52      0.53       936\n",
            "\n",
            "Confusion Matrix\n",
            "\n",
            "[[224  58  49]\n",
            " [ 94 183  79]\n",
            " [ 94  58  97]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate       0.54      0.68      0.60       331\n",
            "     Neutral       0.61      0.51      0.56       356\n",
            "     Counter       0.43      0.39      0.41       249\n",
            "\n",
            "    accuracy                           0.54       936\n",
            "   macro avg       0.53      0.53      0.52       936\n",
            "weighted avg       0.54      0.54      0.53       936\n",
            "\n",
            "Confusion Matrix\n",
            "\n",
            "[[214  70  47]\n",
            " [ 79 213  64]\n",
            " [ 79  72  98]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate       0.58      0.65      0.61       331\n",
            "     Neutral       0.60      0.60      0.60       356\n",
            "     Counter       0.47      0.39      0.43       249\n",
            "\n",
            "    accuracy                           0.56       936\n",
            "   macro avg       0.55      0.55      0.55       936\n",
            "weighted avg       0.56      0.56      0.56       936\n",
            "\n",
            "Confusion Matrix\n",
            "\n",
            "[[198  92  41]\n",
            " [ 61 258  37]\n",
            " [ 68 105  76]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate       0.61      0.60      0.60       331\n",
            "     Neutral       0.57      0.72      0.64       356\n",
            "     Counter       0.49      0.31      0.38       249\n",
            "\n",
            "    accuracy                           0.57       936\n",
            "   macro avg       0.56      0.54      0.54       936\n",
            "weighted avg       0.56      0.57      0.56       936\n",
            "\n",
            "Confusion Matrix\n",
            "\n",
            "[[167  73  91]\n",
            " [ 52 205  99]\n",
            " [ 47  68 134]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate       0.63      0.50      0.56       331\n",
            "     Neutral       0.59      0.58      0.58       356\n",
            "     Counter       0.41      0.54      0.47       249\n",
            "\n",
            "    accuracy                           0.54       936\n",
            "   macro avg       0.54      0.54      0.54       936\n",
            "weighted avg       0.56      0.54      0.54       936\n",
            "\n",
            "Confusion Matrix\n",
            "\n",
            "[[217  65  49]\n",
            " [ 85 212  59]\n",
            " [ 87  69  93]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate       0.56      0.66      0.60       331\n",
            "     Neutral       0.61      0.60      0.60       356\n",
            "     Counter       0.46      0.37      0.41       249\n",
            "\n",
            "    accuracy                           0.56       936\n",
            "   macro avg       0.54      0.54      0.54       936\n",
            "weighted avg       0.55      0.56      0.55       936\n",
            "\n",
            "Confusion Matrix\n",
            "\n",
            "[[206  61  64]\n",
            " [ 72 197  87]\n",
            " [ 71  61 117]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate       0.59      0.62      0.61       331\n",
            "     Neutral       0.62      0.55      0.58       356\n",
            "     Counter       0.44      0.47      0.45       249\n",
            "\n",
            "    accuracy                           0.56       936\n",
            "   macro avg       0.55      0.55      0.55       936\n",
            "weighted avg       0.56      0.56      0.56       936\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [23/23 00:09]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            "\n",
            "[[116  40  28]\n",
            " [ 65 263  33]\n",
            " [ 25  72  71]]\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate       0.56      0.63      0.59       184\n",
            "     Neutral       0.70      0.73      0.71       361\n",
            "     Counter       0.54      0.42      0.47       168\n",
            "\n",
            "    accuracy                           0.63       713\n",
            "   macro avg       0.60      0.59      0.59       713\n",
            "weighted avg       0.63      0.63      0.63       713\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "278"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "trainer.train()\n",
        "f.write(str(trainer.state.log_history))\n",
        "eval=trainer.evaluate(test_dataset)\n",
        "f.write(str(eval))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ryEcqqU74F88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf80fc8a-f715-47a6-8329-dddc7bc1e2c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.9269415736198425,\n",
              " 'eval_accuracy': 0.6311360448807855,\n",
              " 'eval_precision': 0.627148134898186,\n",
              " 'eval_recall': 0.6311360448807855,\n",
              " 'eval_f1': 0.6268915748458709,\n",
              " 'eval_runtime': 9.5147,\n",
              " 'eval_samples_per_second': 74.937,\n",
              " 'eval_steps_per_second': 2.417,\n",
              " 'epoch': 5.0}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "eval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f.close()"
      ],
      "metadata": {
        "id": "oRozWBnxBp2P"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6r7dY6JtUBa1"
      },
      "execution_count": 16,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}